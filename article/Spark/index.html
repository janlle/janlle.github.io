<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          Spark - leone | Blog
        
    </title>

    <link rel="canonical" href="http://www.wealip.cn/article/Spark/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">

    <link rel="stylesheet" href="/css/donate.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('/img/article_header/article_header.png')
            /*post*/
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#大数据" title="大数据">大数据</a>
                            
                              <a class="tag" href="/tags/#Spark" title="Spark">Spark</a>
                            
                        </div>
                        <h1>Spark</h1>
                        <h2 class="subheading">Spark 核心概念及入门</h2>
                        <span class="meta">
                            Posted by leone on
                            2018-12-07
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">leone</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1><span id="spark">Spark</span></h1>
<h2><span id="spark-背景">Spark 背景</span></h2>
<h3><span id="什么是-spark">什么是 Spark</span></h3>
<p>Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。</p>
<h3><span id="spark与hadoop">Spark与Hadoop</span></h3>
<p>Spark是一个计算框架,而Hadoop中包含计算框架MapReduce和分布式文件系统HDFS,Hadoop更广泛地说还包括在其生态系统上的其他系统.</p>
<h3><span id="为什么使用spark">为什么使用Spark?</span></h3>
<p>Hadoop的MapReduce计算模型存在问题:<br>
Hadoop的MapReduce的核心是Shuffle(洗牌).在整个Shuffle的过程中,至少产生6次I/O流.基于MapReduce计算引擎通常会将结果输出到次盘上,进行存储和容错.另外,当一些查询(如:hive)翻译到MapReduce任务是,往往会产生多个Stage,而这些Stage有依赖底层文件系统来存储每一个Stage的输出结果,而I/O的效率往往较低,从而影响MapReduce的运行速度.</p>
<h3><span id="spark的特点-快-易用-通用兼容性">Spark的特点: 快, 易用, 通用,兼容性</span></h3>
<ul>
<li>快：与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。</li>
<li>易用：Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。</li>
<li>通用：Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</li>
<li>兼容性：Spark 可以非常方便地与其他的开源产品进行融合。比如，Spark 可以使用Hadoop 的 YARN 和 Apache Mesos 作为它的资源管理和调度器.并且可以处理所有 Hadoop 支持的数据，包括 HDFS、HBase 和 Cassandra 等。这对于已经部署Hadoop 集群的用户特别重要，因为不需要做任何数据迁移就可以使用 Spark 的强大处理能力。Spark 也可以不依赖于第三方的资源管理和调度器，它实现了Standalone 作为其内置的资源管理和调度框架，这样进一步降低了 Spark 的使用门槛，使得所有人都可以非常容易地部署和使用 Spark。此外，Spark 还提供了在EC2 上部Standalone 的 Spark 集群的工具。</li>
</ul>
<h3><span id="spark的生态系统">Spark的生态系统</span></h3>
<ul>
<li>1.Spark Streaming:<br>
Spark Streaming基于微批量方式的计算和处理,可以用于处理实时的流数据.它使用DStream,简单来说是一个弹性分布式数据集(RDD)系列,处理实时数据.数据可以从Kafka,Flume,Kinesis或TCP套接字等众多来源获取,并且可以使用由高级函数（如 map，reduce，join 和 window）开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。</li>
<li>2.Spark SQL<br>
SPark SQL可以通过JDBC API将Spark数据集暴露出去,而且还可以用传统的BI和可视化工具在Spark数据上执行类似SQL的查询,用户哈可以用Spark SQL对不同格式的数据(如Json, Parque以及数据库等)执行ETl,将其转化,然后暴露特定的查询.</li>
<li>3.Spark MLlib<br>
MLlib是一个可扩展的Spark机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。</li>
<li>4.Spark Graphx:<br>
GraphX是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），一种顶点和边都带有属性的有向多重图，扩展了Spark RDD。为了支持图计算，GraphX暴露了一个基础操作符集合（如subgraph，joinVertices和aggregateMessages）和一个经过优化的Pregel API变体。此外，GraphX还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。</li>
<li>5.Tachyon<br>
Tachyon是一个以内存为中心的分布式文件系统,能够提供内存级别速度的跨集群框架(如Spark和mapReduce)的可信文件共享.它将工作集文件缓存在内存中,从而避免到磁盘中加载需要经常读取的数据集,通过这一机制,不同的作业/查询和框架可以内存级的速度访问缓存文件.<br>
此外，还有一些用于与其他产品集成的适配器，如Cassandra（Spark Cassandra 连接器）和R（SparkR）。Cassandra Connector可用于访问存储在Cassandra数据库中的数据并在这些数据上执行数据分析。</li>
<li>6.Mesos<br>
Mesos是一个资源管理框架<br>
提供类似于YARN的功能<br>
用户可以在其中插件式地运行Spark,MapReduce,Tez等计算框架任务<br>
Mesos对资源和任务进行隔离,并实现高效的资源任务调度</li>
<li>7.BlinkDB<br>
BlinkDB是一个用于在海量数据上进行交互式SQL的近似查询引擎<br>
允许用户通过查询准确性和查询时间之间做出权衡,完成近似查询<br>
核心思想:通过一个自适应优化框架,随着时间的推移,从原始数据建立并维护一组多维样本,通过一个动态样本选择策略,选择一个适当大小的示例,然后基于查询的准确性和响应时间满足用户查询需求</li>
</ul>
<p>除了这些库意外,还有一些其他的库,如Blink和Tachyon.<br>
BlinkDB是一个近似查询 引擎,用于海量数据执行交互式SQL查询.BlinkDB可以通过牺牲数据精度来提升查询响应时间.通过在数据样本上执行查询并展示包含有意义的错误线注解的结果,操作大数据集合.</p>
<p>Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行.<br>
Spark集群部署后,需要在主节点和从节点分别启动master进程和Worker进程,对整个集群进行控制.在一个Spark应用的执行程序中.Driver和Worker是两个重要的角色.Driver程序是应用逻辑执行的起点，负责作业的调度,即Task任务的发布,而多个Worker用来管理计算节点和创建Executor并行处理任务.在执行阶段,Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器.同时Executor对相应数据分区的任务进行处理.</p>
<h3><span id="sparkde架构中的基本组件">Sparkde架构中的基本组件:</span></h3>
<ul>
<li>ClusterManager:在standlone模式中即为Master(主节点),控制整个集群.监控Worker.在Yarn模式中为资源管理器.</li>
<li>Worker:从节点,负责控制计算节点,启动Ex而粗投入或Driver</li>
<li>NodeManager:负责计算节点的控制。</li>
<li>Driver:运行Application的main() 函数并创建SparkContext</li>
<li>Executor: 执行器,在worker node上执行任务组件,用于启动线程执行任务.每个Application拥有独立的一组Executors</li>
<li>SparkContext: 整个应用的上下文,监控应用的生命周期</li>
<li>RDD:弹性分布式集合,spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph</li>
<li>DAG Scheduler: 根据作业(Job)构建基于Stage的DAG,并交给Stage给TaskScheduler</li>
<li>TaskScheduler：将任务（Task）分发给Executor执行</li>
<li>SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。SparkEnv内创建并包含如下一些重要组件的引用。</li>
<li>MapOutPutTracker：负责Shuffle元信息的存储。</li>
<li>BroadcastManager：负责广播变量的控制与元信息的存储。</li>
<li>BlockManager：负责存储管理、创建和查找块。</li>
<li>MetricsSystem：监控运行时性能指标信息。</li>
<li>SparkConf：负责存储配置信息。</li>
<li>Spark的整体流程:client提交应用,Master找到一个Worker启动Driver,Driver向Master或者向资源管理器申请资源,之后将应用转化为RDD Graph，再由DAGScheduler将RDD Graph转化为Stage的有向无环图提交给TaskScheduler，由TaskScheduler提交任务给Executor执行。在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行。</li>
</ul>
<h2><span id="搭建spark集群">搭建spark集群</span></h2>
<blockquote>
<p>安装java环境,spark自动会把scala SDK打包到spark中无需安装scala环境</p>
</blockquote>
<h3><span id="配置spark">配置spark</span></h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cp $SPARK_HOME/conf/spark-env.sh.template spark-env.sh</span><br><span class="line">$ vim $SPARK_HOME/conf/spark-env.sh</span><br><span class="line"></span><br><span class="line">添加</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_191</span><br><span class="line"></span><br><span class="line">#export SPARK_MASTER_IP=node-1</span><br><span class="line">#export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cp $SPARK_HOME/conf/slaves.template slaves</span><br><span class="line"></span><br><span class="line">$ vi slaves</span><br><span class="line"># 在该文件中添加子节点所在的位置（Worker节点）</span><br><span class="line">node-2</span><br><span class="line">node-3</span><br><span class="line">node-4</span><br></pre></td></tr></table></figure>
<h3><span id="启动spark集群">启动spark集群</span></h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/sbin/start-master.sh</span><br><span class="line"></span><br><span class="line">$SPARK_HOME/sbin/start-slaves.sh</span><br></pre></td></tr></table></figure>
<p>启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：<a href="http://node-1:8080/" target="_blank" rel="noopener">http://node-1:8080/</a></p>
<p>到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：</p>
<p>Spark集群规划：node-1，node-2是Master；node-3，node-4，node-5是Worker</p>
<p>安装配置zk集群，并启动zk集群</p>
<p>停止spark所有服务，<a href="http://xn--spark-env-477nh9f666h82an66xy9tc.sh" target="_blank" rel="noopener">修改配置文件spark-env.sh</a>，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置</p>
<p>export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark&quot;</p>
<p>1.在node1节点上修改slaves配置文件内容指定worker节点</p>
<p>2.在node1上执行$SPARK_HOME/sbin/start-all.sh，然后在node2上执行$SPARK_HOME/sbin/start-master.sh启动第二个Master</p>
<h3><span id="执行第一个spark程序">执行第一个spark程序</span></h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://localhost:7077 --executor-memory 1G --total-executor-cores 1 $SPARK_HOME/examples/jars/spark-examples_2.11-2.2.2.jar 100</span><br></pre></td></tr></table></figure>
<h3><span id="spark-shell">spark Shell</span></h3>
<p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/bin/spark-shell \</span><br><span class="line"></span><br><span class="line">--master spark://localhost:7077 \</span><br><span class="line"></span><br><span class="line">--executor-memory 2g \</span><br><span class="line"></span><br><span class="line">--total-executor-cores 2</span><br></pre></td></tr></table></figure>
<p>参数说明：</p>
<p>–master spark://localhost:7077       指定Master的地址</p>
<p>–executor-memory 2g                指定每个worker可用内存为2G</p>
<p>–total-executor-cores 2                指定整个集群使用的cup核数为2个</p>
<p>注意：</p>
<p>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p>
<h3><span id="spark-shell中编写wordcount">spark shell中编写WordCount</span></h3>
<p>在spark shell中用scala语言编写spark程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(&quot;file:///root/tmp/words.dta&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;file:///root/tmp/out&quot;)</span><br></pre></td></tr></table></figure>
<p>说明：</p>
<p>sc是SparkContext对象，该对象时提交spark程序的入口</p>
<p>textFile(“file:///root/tmp/words.dta”)  从本地文件中读取数据</p>
<p>flatMap(_.split(&quot; &quot;))                     先map在压平</p>
<p>map((_,1))                              将单词和1构成元组</p>
<p>reduceByKey(<em>+</em>)                          按照key进行reduce，并将value累加</p>
<p>saveAsTextFile(“file:///root/tmp/out”)  将结果写入到指定位置</p>
<h2><span id="spark-rdd">spark RDD</span></h2>
<h3><span id="rdd概述">RDD概述</span></h3>
<h4><span id="什么是rdd">什么是RDD</span></h4>
<p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p>
<h4><span id="rdd的属性">RDD的属性</span></h4>
<ul>
<li>一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</li>
</ul>
<ul>
<li>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</li>
</ul>
<ul>
<li>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li>
</ul>
<ul>
<li>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</li>
</ul>
<ul>
<li>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
<h4><span id="创建rdd">创建RDD</span></h4>
<ul>
<li>由一个已经存在的Scala集合创建。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://localhost:9000/wc/words.txt"</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="rdd编程模型">RDD编程模型</span></h3>
<h4><span id="spark算子的分类">spark算子的分类</span></h4>
<ul>
<li>
<p>从大方向来说，Spark 算子大致可以分为以下两类:</p>
<p>​     1.Transformation 变换/转换算子：这种变换并不触发提交作业，完成作业中间过程处理。Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。</p>
<p>​     2.Action 行动算子：这类算子会触发 SparkContext 提交 Job 作业。Action 算子会触发 Spark 提交作业（Job），并将数据输出 Spark系统。</p>
</li>
<li>
<p>从小方向来说，Spark 算子大致可以分为以下三类:</p>
<p>​ 1.Value数据类型的Transformation算子，这种变换并不触发提交作业，针对处理的数据项是Value型的数据。<br>
​ 2.Key-Value数据类型的Transfromation算子，这种变换并不触发提交作业，针对处理的数据项是Key-Value型的数据对。</p>
<p>​ 3.Action算子，这类算子会触发SparkContext提交Job作业。</p>
</li>
<li>
<p>Value数据类型的Transformation算子</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>算子</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入分区与输出分区一对一型</td>
<td>1、map算子2、flatMap算子3、mapPartitions算子4、glom算子</td>
</tr>
<tr>
<td>输入分区与输出分区多对一型</td>
<td>5、union算子6、cartesian算子</td>
</tr>
<tr>
<td>输入分区与输出分区多对多型</td>
<td>7、grouBy算子</td>
</tr>
<tr>
<td>输出分区为输入分区子集型</td>
<td>8、filter算子9、distinct算子10、subtract算子11、sample算子12、takeSample算子</td>
</tr>
<tr>
<td>Cache类型</td>
<td>13、cache算子14、persist算子</td>
</tr>
</tbody>
</table>
</li>
</ul>
<ul>
<li>
<p>Key-Value数据类型的Transfromation算子</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>算子</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入分区与输出分区一对一</td>
<td>15、mapValues算子</td>
</tr>
<tr>
<td>对单个RDD或两个RDD聚集</td>
<td>单个RDD聚集16、combineByKey算子17、reduceByKey算子18、partitionBy算子两个RDD聚集19、Cogroup算子</td>
</tr>
<tr>
<td>连接</td>
<td>20、join算子21、leftOutJoin和rightOutJoin算子</td>
</tr>
</tbody>
</table>
</li>
</ul>
<ul>
<li>
<p>Action算子</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>算子</th>
</tr>
</thead>
<tbody>
<tr>
<td>无输出</td>
<td>22、foreach算子</td>
</tr>
<tr>
<td>HDFS</td>
<td>23、saveAsTextFile算子24、saveAsObjectFile算子</td>
</tr>
<tr>
<td>Scala集合和数据类型</td>
<td>25、collect算子26、collectAsMap算子27、reduceByKeyLocally算子28、lookup算子29、count算子30、top算子31、reduce算子32、fold算子33、aggregate算子</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h4><span id="transformation算子详细介绍">Transformation算子详细介绍</span></h4>
<p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。</p>
<p>常用的Transformation：</p>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>转换</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>map</strong>(func)</td>
<td>将原来 RDD 的每个数据项通过 map 中的用户自定义函数 f 映射转变为一个新的元素。源码中 map 算子相当于初始化一个 RDD， 新 RDD 叫做 MappedRDD(this, sc.clean(f))。</td>
</tr>
<tr>
<td style="text-align:left"><strong>filter</strong>(func)</td>
<td>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td>
</tr>
<tr>
<td style="text-align:left"><strong>flatMap</strong>(func)</td>
<td>将原来 RDD 中的每个元素通过函数 f 转换为新的元素，并将生成的 RDD 的每个集合中的元素合并为一个集合，内部创建 FlatMappedRDD(this，sc.clean(f))。</td>
</tr>
<tr>
<td style="text-align:left"><strong>mapPartitions</strong>(func)</td>
<td>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</td>
</tr>
<tr>
<td style="text-align:left"><strong>mapPartitionsWithIndex</strong>(func)</td>
<td>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是   (Int,   Interator[T]) =&gt; Iterator[U]</td>
</tr>
<tr>
<td style="text-align:left"><strong>sample</strong>(withReplacement, fraction, seed)</td>
<td>根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</td>
</tr>
<tr>
<td style="text-align:left"><strong>union</strong>(otherDataset)</td>
<td>对源RDD和参数RDD求并集后返回一个新的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>intersection</strong>(otherDataset)</td>
<td>对源RDD和参数RDD求交集后返回一个新的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>distinct</strong>([numTasks]))</td>
<td>对源RDD进行去重后返回一个新的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>groupByKey</strong>([numTasks])</td>
<td>在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>reduceByKey</strong>(func, [numTasks])</td>
<td>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td>
</tr>
<tr>
<td style="text-align:left"><strong>aggregateByKey</strong>(zeroValue)(seqOp, combOp, [numTasks])</td>
<td>类似reduceByKey，对pairRDD中想用的key值进行聚合操作，使用初始值（seqOp中使用，而combOpenCL中未使用）对应返回值为pairRDD，而区于aggregate（返回值为非RDD）</td>
</tr>
<tr>
<td style="text-align:left"><strong>sortByKey</strong>([ascending], [numTasks])</td>
<td>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>sortBy</strong>(func,[ascending], [numTasks])</td>
<td>与sortByKey类似，但是更灵活</td>
</tr>
<tr>
<td style="text-align:left"><strong>join</strong>(otherDataset, [numTasks])</td>
<td>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>cogroup</strong>(otherDataset, [numTasks])</td>
<td>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></td>
</tr>
<tr>
<td style="text-align:left"><strong>cartesian</strong>(otherDataset)</td>
<td>求笛卡尔乘积。该操作不会执行shuffle操作。</td>
</tr>
<tr>
<td style="text-align:left"><strong>pipe</strong>(command, [envVars])</td>
<td>通过一个shell命令来对RDD各分区进行“管道化”。通过pipe变换将一些shell命令用于Spark中生成的新RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>coalesce</strong>(numPartitions**)**</td>
<td>重新分区，减少RDD中分区的数量到numPartitions</td>
</tr>
<tr>
<td style="text-align:left"><strong>repartition</strong>(numPartitions)</td>
<td>repartition是coalesce接口中shuffle为true的简易实现，即Reshuffle RDD并随机分区，使各分区数据量尽可能平衡。若分区之后分区数远大于原分区数，则需要shuffle。</td>
</tr>
<tr>
<td style="text-align:left"><strong>repartitionAndSortWithinPartitions</strong>(partitioner)</td>
<td>该方法根据partitioner对RDD进行分区，并且在每个结果分区中按key进行排序。</td>
</tr>
</tbody>
</table>
<h4><span id="action算子详细介绍">Action算子详细介绍</span></h4>
<table>
<thead>
<tr>
<th><strong>动作</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>在驱动程序中，以数组的形式返回数据集的所有元素</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回RDD的元素个数</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回RDD的第一个元素（类似于take(1)）</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>返回一个由数据集的前n个元素组成的数组</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>,<em>num</em>, [<em>seed</em>])</td>
<td>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td></td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td></td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>在数据集的每一个元素上，运行函数func进行更新。</td>
</tr>
</tbody>
</table>
<h3><span id="rdd的依赖关系">RDD的依赖关系</span></h3>
<p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p>
<p>shuffle重要的依据：父RDD的一个分区的数据，要给子RDD的多个分区</p>
<h4><span id="窄依赖">窄依赖</span></h4>
<p>窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用</p>
<p>总结：窄依赖我们形象的比喻为独生子女</p>
<h4><span id="宽依赖">宽依赖</span></h4>
<p>宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition</p>
<p>总结：窄依赖我们形象的比喻为超生</p>
<h4><span id="lineage">Lineage</span></h4>
<p>RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<h3><span id="rdd的缓存">RDD的缓存</span></h3>
<p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<p>cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。</p>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/article/jvm/" data-toggle="tooltip" data-placement="top" title="JVM">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/article/Kafka/" data-toggle="tooltip" data-placement="top" title="Kafka">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <br>

                <!--打赏-->
                
                <!--打赏-->

                <br>
                <!--分享-->
                
                    <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                    <!--  css & js -->
                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!--分享-->
                <br>                       
                
                <!-- require APlayer -->
                

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->

                

            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

  
    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.</span> <span class="toc-nav-text"><span id="spark">Spark</span></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text"><span id="spark-&#x80CC;&#x666F;">Spark &#x80CC;&#x666F;</span></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.1.1.</span> <span class="toc-nav-text"><span id="&#x4EC0;&#x4E48;&#x662F;-spark">&#x4EC0;&#x4E48;&#x662F; Spark</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.1.2.</span> <span class="toc-nav-text"><span id="spark&#x4E0E;hadoop">Spark&#x4E0E;Hadoop</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.1.3.</span> <span class="toc-nav-text"><span id="&#x4E3A;&#x4EC0;&#x4E48;&#x4F7F;&#x7528;spark">&#x4E3A;&#x4EC0;&#x4E48;&#x4F7F;&#x7528;Spark?</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.1.4.</span> <span class="toc-nav-text"><span id="spark&#x7684;&#x7279;&#x70B9;-&#x5FEB;-&#x6613;&#x7528;-&#x901A;&#x7528;&#x517C;&#x5BB9;&#x6027;">Spark&#x7684;&#x7279;&#x70B9;: &#x5FEB;, &#x6613;&#x7528;, &#x901A;&#x7528;,&#x517C;&#x5BB9;&#x6027;</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.1.5.</span> <span class="toc-nav-text"><span id="spark&#x7684;&#x751F;&#x6001;&#x7CFB;&#x7EDF;">Spark&#x7684;&#x751F;&#x6001;&#x7CFB;&#x7EDF;</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.1.6.</span> <span class="toc-nav-text"><span id="sparkde&#x67B6;&#x6784;&#x4E2D;&#x7684;&#x57FA;&#x672C;&#x7EC4;&#x4EF6;">Sparkde&#x67B6;&#x6784;&#x4E2D;&#x7684;&#x57FA;&#x672C;&#x7EC4;&#x4EF6;:</span></span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.2.</span> <span class="toc-nav-text"><span id="&#x642D;&#x5EFA;spark&#x96C6;&#x7FA4;">&#x642D;&#x5EFA;spark&#x96C6;&#x7FA4;</span></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.2.1.</span> <span class="toc-nav-text"><span id="&#x914D;&#x7F6E;spark">&#x914D;&#x7F6E;spark</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.2.2.</span> <span class="toc-nav-text"><span id="&#x542F;&#x52A8;spark&#x96C6;&#x7FA4;">&#x542F;&#x52A8;spark&#x96C6;&#x7FA4;</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.2.3.</span> <span class="toc-nav-text"><span id="&#x6267;&#x884C;&#x7B2C;&#x4E00;&#x4E2A;spark&#x7A0B;&#x5E8F;">&#x6267;&#x884C;&#x7B2C;&#x4E00;&#x4E2A;spark&#x7A0B;&#x5E8F;</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.2.4.</span> <span class="toc-nav-text"><span id="spark-shell">spark Shell</span></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.2.5.</span> <span class="toc-nav-text"><span id="spark-shell&#x4E2D;&#x7F16;&#x5199;wordcount">spark shell&#x4E2D;&#x7F16;&#x5199;WordCount</span></span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.</span> <span class="toc-nav-text"><span id="spark-rdd">spark RDD</span></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.1.</span> <span class="toc-nav-text"><span id="rdd&#x6982;&#x8FF0;">RDD&#x6982;&#x8FF0;</span></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.1.1.</span> <span class="toc-nav-text"><span id="&#x4EC0;&#x4E48;&#x662F;rdd">&#x4EC0;&#x4E48;&#x662F;RDD</span></span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.1.2.</span> <span class="toc-nav-text"><span id="rdd&#x7684;&#x5C5E;&#x6027;">RDD&#x7684;&#x5C5E;&#x6027;</span></span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.1.3.</span> <span class="toc-nav-text"><span id="&#x521B;&#x5EFA;rdd">&#x521B;&#x5EFA;RDD</span></span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.2.</span> <span class="toc-nav-text"><span id="rdd&#x7F16;&#x7A0B;&#x6A21;&#x578B;">RDD&#x7F16;&#x7A0B;&#x6A21;&#x578B;</span></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.2.1.</span> <span class="toc-nav-text"><span id="spark&#x7B97;&#x5B50;&#x7684;&#x5206;&#x7C7B;">spark&#x7B97;&#x5B50;&#x7684;&#x5206;&#x7C7B;</span></span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.2.2.</span> <span class="toc-nav-text"><span id="transformation&#x7B97;&#x5B50;&#x8BE6;&#x7EC6;&#x4ECB;&#x7ECD;">Transformation&#x7B97;&#x5B50;&#x8BE6;&#x7EC6;&#x4ECB;&#x7ECD;</span></span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.2.3.</span> <span class="toc-nav-text"><span id="action&#x7B97;&#x5B50;&#x8BE6;&#x7EC6;&#x4ECB;&#x7ECD;">Action&#x7B97;&#x5B50;&#x8BE6;&#x7EC6;&#x4ECB;&#x7ECD;</span></span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.3.</span> <span class="toc-nav-text"><span id="rdd&#x7684;&#x4F9D;&#x8D56;&#x5173;&#x7CFB;">RDD&#x7684;&#x4F9D;&#x8D56;&#x5173;&#x7CFB;</span></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.3.1.</span> <span class="toc-nav-text"><span id="&#x7A84;&#x4F9D;&#x8D56;">&#x7A84;&#x4F9D;&#x8D56;</span></span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.3.2.</span> <span class="toc-nav-text"><span id="&#x5BBD;&#x4F9D;&#x8D56;">&#x5BBD;&#x4F9D;&#x8D56;</span></span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.3.3.</span> <span class="toc-nav-text"><span id="lineage">Lineage</span></span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#undefined"><span class="toc-nav-number">1.3.4.</span> <span class="toc-nav-text"><span id="rdd&#x7684;&#x7F13;&#x5B58;">RDD&#x7684;&#x7F13;&#x5B58;</span></span></a></li></ol></li></ol></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#大数据" title="大数据">大数据</a>
                        
                          <a class="tag" href="/tags/#Spark" title="Spark">Spark</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="http://blog.csdn.net/fooelliot" target="_blank">CSDN Blog leone</a></li>
                    
                        <li><a href="http://dockone.io/people/leone" target="_blank">DockOne leone</a></li>
                    
                        <li><a href="https://yq.aliyun.com/u/leone" target="_blank">阿里云栖社区 leone</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/janlle">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; leone 2019 
                    <br>
                    Theme by <a href="http://beantech.org">BeanTech</a> 
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span> 
                    re-Ported by <a href="http://wealip.cn">leone</a> | 
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=janlle&repo=blogs&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://www.wealip.cn/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'xxx';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="http://www.wealip.cn/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
