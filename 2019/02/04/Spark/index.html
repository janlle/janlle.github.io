<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="sparkspark背景什么是sparkSpark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/2019/02/04/Spark/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="sparkspark背景什么是sparkSpark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-01-20T15:17:03.341Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="sparkspark背景什么是sparkSpark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Spark" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/02/04/Spark/" class="article-date">
  <time datetime="2019-02-04T12:16:14.124Z" itemprop="datePublished">2019-02-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h1><h2 id="spark背景"><a href="#spark背景" class="headerlink" title="spark背景"></a>spark背景</h2><h3 id="什么是spark"><a href="#什么是spark" class="headerlink" title="什么是spark"></a>什么是spark</h3><p>Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。</p>
<h3 id="Spark与Hadoop"><a href="#Spark与Hadoop" class="headerlink" title="Spark与Hadoop"></a>Spark与Hadoop</h3><p>Spark是一个计算框架,而Hadoop中包含计算框架MapReduce和分布式文件系统HDFS,Hadoop更广泛地说还包括在其生态系统上的其他系统.</p>
<h3 id="为什么使用Spark"><a href="#为什么使用Spark" class="headerlink" title="为什么使用Spark?"></a>为什么使用Spark?</h3><p>Hadoop的MapReduce计算模型存在问题:<br>Hadoop的MapReduce的核心是Shuffle(洗牌).在整个Shuffle的过程中,至少产生6次I/O流.基于MapReduce计算引擎通常会将结果输出到次盘上,进行存储和容错.另外,当一些查询(如:hive)翻译到MapReduce任务是,往往会产生多个Stage,而这些Stage有依赖底层文件系统来存储每一个Stage的输出结果,而I/O的效率往往较低,从而影响MapReduce的运行速度.</p>
<h3 id="Spark的特点-快-易用-通用-兼容性"><a href="#Spark的特点-快-易用-通用-兼容性" class="headerlink" title="Spark的特点: 快, 易用, 通用,兼容性"></a>Spark的特点: 快, 易用, 通用,兼容性</h3><ul>
<li>快：与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。</li>
<li>易用：Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。</li>
<li>通用：Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</li>
<li>兼容性：Spark 可以非常方便地与其他的开源产品进行融合。比如，Spark 可以使用Hadoop 的 YARN 和 Apache Mesos 作为它的资源管理和调度器.并且可以处理所有 Hadoop 支持的数据，包括 HDFS、HBase 和 Cassandra 等。这对于已经部署Hadoop 集群的用户特别重要，因为不需要做任何数据迁移就可以使用 Spark 的强大处理能力。Spark 也可以不依赖于第三方的资源管理和调度器，它实现了Standalone 作为其内置的资源管理和调度框架，这样进一步降低了 Spark 的使用门槛，使得所有人都可以非常容易地部署和使用 Spark。此外，Spark 还提供了在EC2 上部Standalone 的 Spark 集群的工具。</li>
</ul>
<h3 id="Spark的生态系统"><a href="#Spark的生态系统" class="headerlink" title="Spark的生态系统"></a>Spark的生态系统</h3><ul>
<li>1.Spark Streaming:<br>Spark Streaming基于微批量方式的计算和处理,可以用于处理实时的流数据.它使用DStream,简单来说是一个弹性分布式数据集(RDD)系列,处理实时数据.数据可以从Kafka,Flume,Kinesis或TCP套接字等众多来源获取,并且可以使用由高级函数（如 map，reduce，join 和 window）开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。</li>
<li>2.Spark SQL<br>SPark SQL可以通过JDBC API将Spark数据集暴露出去,而且还可以用传统的BI和可视化工具在Spark数据上执行类似SQL的查询,用户哈可以用Spark SQL对不同格式的数据(如Json, Parque以及数据库等)执行ETl,将其转化,然后暴露特定的查询.</li>
<li>3.Spark MLlib<br>MLlib是一个可扩展的Spark机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。</li>
<li>4.Spark Graphx:<br>GraphX是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），一种顶点和边都带有属性的有向多重图，扩展了Spark RDD。为了支持图计算，GraphX暴露了一个基础操作符集合（如subgraph，joinVertices和aggregateMessages）和一个经过优化的Pregel API变体。此外，GraphX还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。</li>
<li>5.Tachyon<br>Tachyon是一个以内存为中心的分布式文件系统,能够提供内存级别速度的跨集群框架(如Spark和mapReduce)的可信文件共享.它将工作集文件缓存在内存中,从而避免到磁盘中加载需要经常读取的数据集,通过这一机制,不同的作业/查询和框架可以内存级的速度访问缓存文件.<br>此外，还有一些用于与其他产品集成的适配器，如Cassandra（Spark Cassandra 连接器）和R（SparkR）。Cassandra Connector可用于访问存储在Cassandra数据库中的数据并在这些数据上执行数据分析。</li>
<li>6.Mesos<br>Mesos是一个资源管理框架<br>提供类似于YARN的功能<br>用户可以在其中插件式地运行Spark,MapReduce,Tez等计算框架任务<br>Mesos对资源和任务进行隔离,并实现高效的资源任务调度</li>
<li>7.BlinkDB<br>BlinkDB是一个用于在海量数据上进行交互式SQL的近似查询引擎<br>允许用户通过查询准确性和查询时间之间做出权衡,完成近似查询<br>核心思想:通过一个自适应优化框架,随着时间的推移,从原始数据建立并维护一组多维样本,通过一个动态样本选择策略,选择一个适当大小的示例,然后基于查询的准确性和响应时间满足用户查询需求</li>
</ul>
<p>除了这些库意外,还有一些其他的库,如Blink和Tachyon.<br>BlinkDB是一个近似查询 引擎,用于海量数据执行交互式SQL查询.BlinkDB可以通过牺牲数据精度来提升查询响应时间.通过在数据样本上执行查询并展示包含有意义的错误线注解的结果,操作大数据集合.</p>
<p>Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行.<br>Spark集群部署后,需要在主节点和从节点分别启动master进程和Worker进程,对整个集群进行控制.在一个Spark应用的执行程序中.Driver和Worker是两个重要的角色.Driver程序是应用逻辑执行的起点，负责作业的调度,即Task任务的发布,而多个Worker用来管理计算节点和创建Executor并行处理任务.在执行阶段,Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器.同时Executor对相应数据分区的任务进行处理.</p>
<h3 id="Sparkde架构中的基本组件"><a href="#Sparkde架构中的基本组件" class="headerlink" title="Sparkde架构中的基本组件:"></a>Sparkde架构中的基本组件:</h3><ul>
<li>ClusterManager:在standlone模式中即为Master(主节点),控制整个集群.监控Worker.在Yarn模式中为资源管理器.</li>
<li>Worker:从节点,负责控制计算节点,启动Ex而粗投入或Driver</li>
<li>NodeManager:负责计算节点的控制。</li>
<li>Driver:运行Application的main() 函数并创建SparkContext</li>
<li>Executor: 执行器,在worker node上执行任务组件,用于启动线程执行任务.每个Application拥有独立的一组Executors</li>
<li>SparkContext: 整个应用的上下文,监控应用的生命周期</li>
<li>RDD:弹性分布式集合,spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph</li>
<li>DAG Scheduler: 根据作业(Job)构建基于Stage的DAG,并交给Stage给TaskScheduler</li>
<li>TaskScheduler：将任务（Task）分发给Executor执行 </li>
<li>SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。SparkEnv内创建并包含如下一些重要组件的引用。</li>
<li>MapOutPutTracker：负责Shuffle元信息的存储。</li>
<li>BroadcastManager：负责广播变量的控制与元信息的存储。</li>
<li>BlockManager：负责存储管理、创建和查找块。</li>
<li>MetricsSystem：监控运行时性能指标信息。</li>
<li>SparkConf：负责存储配置信息。</li>
<li>Spark的整体流程:client提交应用,Master找到一个Worker启动Driver,Driver向Master或者向资源管理器申请资源,之后将应用转化为RDD Graph，再由DAGScheduler将RDD Graph转化为Stage的有向无环图提交给TaskScheduler，由TaskScheduler提交任务给Executor执行。在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行。</li>
</ul>
<h2 id="搭建spark集群"><a href="#搭建spark集群" class="headerlink" title="搭建spark集群"></a>搭建spark集群</h2><blockquote>
<p>安装java环境,spark自动会把scala SDK打包到spark中无需安装scala环境</p>
</blockquote>
<h3 id="配置spark"><a href="#配置spark" class="headerlink" title="配置spark"></a>配置spark</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cp $SPARK_HOME/conf/spark-env.sh.template spark-env.sh</span><br><span class="line">$ vim $SPARK_HOME/conf/spark-env.sh</span><br><span class="line"></span><br><span class="line">添加</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_191</span><br><span class="line"></span><br><span class="line">#export SPARK_MASTER_IP=node-1</span><br><span class="line">#export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cp $SPARK_HOME/conf/slaves.template slaves</span><br><span class="line"></span><br><span class="line">$ vi slaves</span><br><span class="line"># 在该文件中添加子节点所在的位置（Worker节点）</span><br><span class="line">node-2</span><br><span class="line">node-3</span><br><span class="line">node-4</span><br></pre></td></tr></table></figure>
<h3 id="启动spark集群"><a href="#启动spark集群" class="headerlink" title="启动spark集群"></a>启动spark集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/sbin/start-master.sh</span><br><span class="line"></span><br><span class="line">$SPARK_HOME/sbin/start-slaves.sh</span><br></pre></td></tr></table></figure>
<p>启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：<a href="http://node-1:8080/" target="_blank" rel="noopener">http://node-1:8080/</a></p>
<p>到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：</p>
<p>Spark集群规划：node-1，node-2是Master；node-3，node-4，node-5是Worker</p>
<p>安装配置zk集群，并启动zk集群</p>
<p>停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置</p>
<p>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark”</p>
<p>1.在node1节点上修改slaves配置文件内容指定worker节点</p>
<p>2.在node1上执行$SPARK_HOME/sbin/start-all.sh，然后在node2上执行$SPARK_HOME/sbin/start-master.sh启动第二个Master</p>
<h3 id="执行第一个spark程序"><a href="#执行第一个spark程序" class="headerlink" title="执行第一个spark程序"></a>执行第一个spark程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://localhost:7077 --executor-memory 1G --total-executor-cores 1 $SPARK_HOME/examples/jars/spark-examples_2.11-2.2.2.jar 100</span><br></pre></td></tr></table></figure>
<h3 id="spark-Shell"><a href="#spark-Shell" class="headerlink" title="spark Shell"></a>spark Shell</h3><p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/bin/spark-shell \</span><br><span class="line"></span><br><span class="line">--master spark://localhost:7077 \</span><br><span class="line"></span><br><span class="line">--executor-memory 2g \</span><br><span class="line"></span><br><span class="line">--total-executor-cores 2</span><br></pre></td></tr></table></figure>
<p>参数说明：</p>
<p>–master spark://localhost:7077       指定Master的地址</p>
<p>–executor-memory 2g                指定每个worker可用内存为2G</p>
<p>–total-executor-cores 2                指定整个集群使用的cup核数为2个</p>
<p>注意：</p>
<p>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p>
<h3 id="spark-shell中编写WordCount"><a href="#spark-shell中编写WordCount" class="headerlink" title="spark shell中编写WordCount"></a>spark shell中编写WordCount</h3><p>在spark shell中用scala语言编写spark程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(&quot;file:///root/tmp/words.dta&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;file:///root/tmp/out&quot;)</span><br></pre></td></tr></table></figure>
<p>说明：</p>
<p>sc是SparkContext对象，该对象时提交spark程序的入口</p>
<p>textFile(“file:///root/tmp/words.dta”)  从本地文件中读取数据</p>
<p>flatMap(_.split(“ “))                     先map在压平</p>
<p>map((_,1))                              将单词和1构成元组</p>
<p>reduceByKey(_+_)                          按照key进行reduce，并将value累加</p>
<p>saveAsTextFile(“file:///root/tmp/out”)  将结果写入到指定位置</p>
<h2 id="spark-RDD"><a href="#spark-RDD" class="headerlink" title="spark RDD"></a>spark RDD</h2><h3 id="RDD概述"><a href="#RDD概述" class="headerlink" title="RDD概述"></a>RDD概述</h3><h4 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h4><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p>
<h4 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h4><ul>
<li>一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</li>
</ul>
<ul>
<li>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</li>
</ul>
<ul>
<li>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li>
</ul>
<ul>
<li>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</li>
</ul>
<ul>
<li>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
<h4 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h4><ul>
<li>由一个已经存在的Scala集合创建。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://localhost:9000/wc/words.txt"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="RDD编程模型"><a href="#RDD编程模型" class="headerlink" title="RDD编程模型"></a>RDD编程模型</h3><h4 id="spark算子的分类"><a href="#spark算子的分类" class="headerlink" title="spark算子的分类"></a>spark算子的分类</h4><ul>
<li><p>从大方向来说，Spark 算子大致可以分为以下两类:</p>
<p>​     1.Transformation 变换/转换算子：这种变换并不触发提交作业，完成作业中间过程处理。Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。</p>
<p>​     2.Action 行动算子：这类算子会触发 SparkContext 提交 Job 作业。Action 算子会触发 Spark 提交作业（Job），并将数据输出 Spark系统。</p>
</li>
<li><p>从小方向来说，Spark 算子大致可以分为以下三类:</p>
<p>​ 1.Value数据类型的Transformation算子，这种变换并不触发提交作业，针对处理的数据项是Value型的数据。<br>​ 2.Key-Value数据类型的Transfromation算子，这种变换并不触发提交作业，针对处理的数据项是Key-Value型的数据对。</p>
<p>​ 3.Action算子，这类算子会触发SparkContext提交Job作业。</p>
</li>
<li><p>Value数据类型的Transformation算子</p>
<p>| 类型 | 算子 |<br>| —— | —— |<br>| 输入分区与输出分区一对一型 | 1、map算子2、flatMap算子3、mapPartitions算子4、glom算子      |<br>| 输入分区与输出分区多对一型 | 5、union算子6、cartesian算子                                 |<br>| 输入分区与输出分区多对多型 | 7、grouBy算子                                                |<br>| 输出分区为输入分区子集型   | 8、filter算子9、distinct算子10、subtract算子11、sample算子12、takeSample算子 |<br>| Cache类型                  | 13、cache算子14、persist算子                                 |</p>
</li>
</ul>
<ul>
<li><p>Key-Value数据类型的Transfromation算子</p>
<p>| 类型                     | 算子                                                         |<br>| ———————— | ———————————————————— |<br>| 输入分区与输出分区一对一 | 15、mapValues算子                                            |<br>| 对单个RDD或两个RDD聚集   | 单个RDD聚集16、combineByKey算子17、reduceByKey算子18、partitionBy算子两个RDD聚集19、Cogroup算子 |<br>| 连接                     | 20、join算子21、leftOutJoin和rightOutJoin算子                |</p>
</li>
</ul>
<ul>
<li><p>Action算子</p>
<p>| 类型                | 算子                                                         |<br>| ——————- | ———————————————————— |<br>| 无输出              | 22、foreach算子                                              |<br>| HDFS                | 23、saveAsTextFile算子24、saveAsObjectFile算子               |<br>| Scala集合和数据类型 | 25、collect算子26、collectAsMap算子27、reduceByKeyLocally算子28、lookup算子29、count算子30、top算子31、reduce算子32、fold算子33、aggregate算子 |</p>
</li>
</ul>
<h4 id="Transformation算子详细介绍"><a href="#Transformation算子详细介绍" class="headerlink" title="Transformation算子详细介绍"></a>Transformation算子详细介绍</h4><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。</p>
<p>常用的Transformation：</p>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>转换</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>map</strong>(func)</td>
<td>将原来 RDD 的每个数据项通过 map 中的用户自定义函数 f 映射转变为一个新的元素。源码中 map 算子相当于初始化一个 RDD， 新 RDD 叫做 MappedRDD(this, sc.clean(f))。</td>
</tr>
<tr>
<td style="text-align:left"><strong>filter</strong>(func)</td>
<td>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td>
</tr>
<tr>
<td style="text-align:left"><strong>flatMap</strong>(func)</td>
<td>将原来 RDD 中的每个元素通过函数 f 转换为新的元素，并将生成的 RDD 的每个集合中的元素合并为一个集合，内部创建 FlatMappedRDD(this，sc.clean(f))。</td>
</tr>
<tr>
<td style="text-align:left"><strong>mapPartitions</strong>(func)</td>
<td>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</td>
</tr>
<tr>
<td style="text-align:left"><strong>mapPartitionsWithIndex</strong>(func)</td>
<td>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是   (Int,   Interator[T]) =&gt; Iterator[U]</td>
</tr>
<tr>
<td style="text-align:left"><strong>sample</strong>(withReplacement, fraction, seed)</td>
<td>根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</td>
</tr>
<tr>
<td style="text-align:left"><strong>union</strong>(otherDataset)</td>
<td>对源RDD和参数RDD求并集后返回一个新的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>intersection</strong>(otherDataset)</td>
<td>对源RDD和参数RDD求交集后返回一个新的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>distinct</strong>([numTasks]))</td>
<td>对源RDD进行去重后返回一个新的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>groupByKey</strong>([numTasks])</td>
<td>在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>reduceByKey</strong>(func, [numTasks])</td>
<td>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td>
</tr>
<tr>
<td style="text-align:left"><strong>aggregateByKey</strong>(zeroValue)(seqOp, combOp, [numTasks])</td>
<td>类似reduceByKey，对pairRDD中想用的key值进行聚合操作，使用初始值（seqOp中使用，而combOpenCL中未使用）对应返回值为pairRDD，而区于aggregate（返回值为非RDD）</td>
</tr>
<tr>
<td style="text-align:left"><strong>sortByKey</strong>([ascending], [numTasks])</td>
<td>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>sortBy</strong>(func,[ascending], [numTasks])</td>
<td>与sortByKey类似，但是更灵活</td>
</tr>
<tr>
<td style="text-align:left"><strong>join</strong>(otherDataset, [numTasks])</td>
<td>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>cogroup</strong>(otherDataset, [numTasks])</td>
<td>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></td>
</tr>
<tr>
<td style="text-align:left"><strong>cartesian</strong>(otherDataset)</td>
<td>求笛卡尔乘积。该操作不会执行shuffle操作。</td>
</tr>
<tr>
<td style="text-align:left"><strong>pipe</strong>(command, [envVars])</td>
<td>通过一个shell命令来对RDD各分区进行“管道化”。通过pipe变换将一些shell命令用于Spark中生成的新RDD</td>
</tr>
<tr>
<td style="text-align:left"><strong>coalesce</strong>(numPartitions<strong>)</strong></td>
<td>重新分区，减少RDD中分区的数量到numPartitions</td>
</tr>
<tr>
<td style="text-align:left"><strong>repartition</strong>(numPartitions)</td>
<td>repartition是coalesce接口中shuffle为true的简易实现，即Reshuffle RDD并随机分区，使各分区数据量尽可能平衡。若分区之后分区数远大于原分区数，则需要shuffle。</td>
</tr>
<tr>
<td style="text-align:left"><strong>repartitionAndSortWithinPartitions</strong>(partitioner)</td>
<td>该方法根据partitioner对RDD进行分区，并且在每个结果分区中按key进行排序。</td>
</tr>
</tbody>
</table>
<h4 id="Action算子详细介绍"><a href="#Action算子详细介绍" class="headerlink" title="Action算子详细介绍"></a>Action算子详细介绍</h4><table>
<thead>
<tr>
<th><strong>动作</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>在驱动程序中，以数组的形式返回数据集的所有元素</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回RDD的元素个数</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回RDD的第一个元素（类似于take(1)）</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>返回一个由数据集的前n个元素组成的数组</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>,<em>num</em>, [<em>seed</em>])</td>
<td>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td></td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td></td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>在数据集的每一个元素上，运行函数func进行更新。</td>
</tr>
</tbody>
</table>
<h3 id="RDD的依赖关系"><a href="#RDD的依赖关系" class="headerlink" title="RDD的依赖关系"></a>RDD的依赖关系</h3><p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p>
<p>shuffle重要的依据：父RDD的一个分区的数据，要给子RDD的多个分区</p>
<h4 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h4><p>窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用</p>
<p>总结：窄依赖我们形象的比喻为独生子女</p>
<h4 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h4><p>宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition</p>
<p>总结：窄依赖我们形象的比喻为超生</p>
<h4 id="Lineage"><a href="#Lineage" class="headerlink" title="Lineage"></a>Lineage</h4><p>RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<h3 id="RDD的缓存"><a href="#RDD的缓存" class="headerlink" title="RDD的缓存"></a>RDD的缓存</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<p>cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/02/04/Spark/" data-id="cjrqbftzd0016akc47vhcpxwt" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/02/04/SpringBoot-00-template/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2019/02/04/Shadowsocks/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/02/04/Crontab/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/02/04/CentOS-7.5-init/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/02/04/bash/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/02/04/awk/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/02/04/后台项目技术规范/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>